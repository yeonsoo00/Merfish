/home/yec23006/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/yec23006/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/yec23006/.conda/envs/torch121/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
Epoch [1/200], Loss: 1714.5540
Epoch [2/200], Loss: 480.4637
Epoch [3/200], Loss: 479.9897
Epoch [4/200], Loss: 479.1483
Epoch [5/200], Loss: 478.8616
Epoch [6/200], Loss: 478.7802
Epoch [7/200], Loss: 478.6654
Epoch [8/200], Loss: 478.6736
Epoch [9/200], Loss: 478.5454
Epoch [10/200], Loss: 478.3501
Epoch [11/200], Loss: 478.3875
Epoch [12/200], Loss: 478.2219
Epoch [13/200], Loss: 478.1818
Epoch [14/200], Loss: 478.1804
Epoch [15/200], Loss: 478.2202
Epoch [16/200], Loss: 478.1324
Epoch [17/200], Loss: 478.1235
Epoch [18/200], Loss: 478.0814
Epoch [19/200], Loss: 478.0011
Epoch [20/200], Loss: 477.9776
Epoch [21/200], Loss: 477.9467
Epoch [22/200], Loss: 477.8509
Epoch [23/200], Loss: 477.8398
Epoch [24/200], Loss: 477.8639
Epoch [25/200], Loss: 477.8280
Epoch [26/200], Loss: 477.8278
Epoch [27/200], Loss: 477.7988
[34m[1mwandb[39m[22m: Network error resolved after 0:00:21.230267, resuming normal operation.
Epoch [28/200], Loss: 477.7528
Epoch [29/200], Loss: 477.7706
Epoch [30/200], Loss: 477.7642
Epoch [31/200], Loss: 477.7536
Epoch [32/200], Loss: 477.6837
Epoch [33/200], Loss: 477.7105
Epoch [34/200], Loss: 477.7245
Epoch [35/200], Loss: 477.7000
Epoch [36/200], Loss: 477.7317
Epoch [37/200], Loss: 477.6325
Epoch [38/200], Loss: 477.5977
Epoch [39/200], Loss: 477.5932
Epoch [40/200], Loss: 477.6153
Epoch [41/200], Loss: 477.5457
Epoch [42/200], Loss: 477.6207
Epoch [43/200], Loss: 477.5711
Epoch [44/200], Loss: 477.5265
Epoch [45/200], Loss: 477.5589
Epoch [46/200], Loss: 477.5150
Epoch [47/200], Loss: 477.4938
Epoch [48/200], Loss: 477.5026
Epoch [49/200], Loss: 477.5185
Epoch [50/200], Loss: 477.4468
Epoch [51/200], Loss: 477.4769
Epoch [52/200], Loss: 477.4733
Epoch [53/200], Loss: 477.4523
Epoch [54/200], Loss: 477.4546
Epoch [55/200], Loss: 477.4713
Epoch [56/200], Loss: 477.4245
Epoch [57/200], Loss: 477.4059
Epoch [58/200], Loss: 477.4092
Epoch [59/200], Loss: 477.3671
Epoch [60/200], Loss: 477.3865
Epoch [61/200], Loss: 477.4099
Epoch [62/200], Loss: 477.3726
Epoch [63/200], Loss: 477.3889
Epoch [64/200], Loss: 477.3400
Epoch [65/200], Loss: 477.3592
Epoch [66/200], Loss: 477.3518
Epoch [67/200], Loss: 477.3626
Epoch [68/200], Loss: 477.3118
Epoch [69/200], Loss: 477.3544
Epoch [70/200], Loss: 477.3305
Epoch [71/200], Loss: 477.2705
Epoch [72/200], Loss: 477.3095
Epoch [73/200], Loss: 477.2930
Epoch [74/200], Loss: 477.2494
Epoch [75/200], Loss: 477.2400
Epoch [76/200], Loss: 477.2211
Epoch [77/200], Loss: 477.2506
Epoch [78/200], Loss: 477.2372
Epoch [79/200], Loss: 477.2467
Epoch [80/200], Loss: 477.2223
Epoch [81/200], Loss: 477.2123
Epoch [82/200], Loss: 477.1871
Epoch [83/200], Loss: 477.1762
Epoch [84/200], Loss: 477.1755
Epoch [85/200], Loss: 477.2026
Epoch [86/200], Loss: 477.1637
Epoch [87/200], Loss: 477.1341
Epoch [88/200], Loss: 477.1007
Epoch [89/200], Loss: 477.1044
Epoch [90/200], Loss: 477.1734
Epoch [91/200], Loss: 477.1466
Epoch [92/200], Loss: 477.0713
Epoch [93/200], Loss: 477.0491
Epoch [94/200], Loss: 477.0836
Epoch [95/200], Loss: 477.1221
Epoch [96/200], Loss: 477.0413
Epoch [97/200], Loss: 477.0129
Epoch [98/200], Loss: 477.0561
Epoch [99/200], Loss: 477.0818
Epoch [100/200], Loss: 477.0499
Epoch [101/200], Loss: 477.0219
Epoch [102/200], Loss: 476.9692
Epoch [103/200], Loss: 476.9846
Epoch [104/200], Loss: 477.0531
Epoch [105/200], Loss: 477.0485
Epoch [106/200], Loss: 476.9770
Epoch [107/200], Loss: 476.9824
Epoch [108/200], Loss: 476.9289
Epoch [109/200], Loss: 476.8994
Epoch [110/200], Loss: 476.9230
Epoch [111/200], Loss: 476.9150
Epoch [112/200], Loss: 476.9736
Epoch [113/200], Loss: 476.9297
Epoch [114/200], Loss: 476.9273
Epoch [115/200], Loss: 476.8594
Epoch [116/200], Loss: 476.8850
Epoch [117/200], Loss: 476.8780
Epoch [118/200], Loss: 476.8812
Epoch [119/200], Loss: 476.8157
Epoch [120/200], Loss: 476.8219
Epoch [121/200], Loss: 476.8064
Epoch [122/200], Loss: 476.8276
Epoch [123/200], Loss: 476.7946
Epoch [124/200], Loss: 476.7916
Epoch [125/200], Loss: 476.7604
Epoch [126/200], Loss: 476.7871
Epoch [127/200], Loss: 476.7768
Epoch [128/200], Loss: 476.8089
Epoch [129/200], Loss: 476.7462
Epoch [130/200], Loss: 476.7581
Epoch [131/200], Loss: 476.6919
Epoch [132/200], Loss: 476.6659
Epoch [133/200], Loss: 476.6396
Epoch [134/200], Loss: 476.6815
Epoch [135/200], Loss: 476.6874
Epoch [136/200], Loss: 476.6351
Epoch [137/200], Loss: 476.6472
Epoch [138/200], Loss: 476.6868
Epoch [139/200], Loss: 476.5448
Epoch [140/200], Loss: 476.6378
Epoch [141/200], Loss: 476.6628
Epoch [142/200], Loss: 476.6966
Epoch [143/200], Loss: 476.6162
Epoch [144/200], Loss: 476.6180
Epoch [145/200], Loss: 476.5574
Epoch [146/200], Loss: 476.5838
Epoch [147/200], Loss: 476.6349
Epoch [148/200], Loss: 476.5756
Epoch [149/200], Loss: 476.5265
Epoch [150/200], Loss: 476.5808
Epoch [151/200], Loss: 476.5047
Epoch [152/200], Loss: 476.4894
Epoch [153/200], Loss: 476.4850
Epoch [154/200], Loss: 476.5921
Epoch [155/200], Loss: 476.5265
Epoch [156/200], Loss: 476.5401
Epoch [157/200], Loss: 476.5256
Epoch [158/200], Loss: 476.5161
Epoch [159/200], Loss: 476.5122
Epoch [160/200], Loss: 476.5262
Epoch [161/200], Loss: 476.4615
Epoch [162/200], Loss: 476.4145
Epoch [163/200], Loss: 476.4301
Epoch [164/200], Loss: 476.3811
Epoch [165/200], Loss: 476.3503
Epoch [166/200], Loss: 476.3648
Epoch [167/200], Loss: 476.3961
Epoch [168/200], Loss: 476.3714
Epoch [169/200], Loss: 476.4260
Epoch [170/200], Loss: 476.3571
Epoch [171/200], Loss: 476.2732
Epoch [172/200], Loss: 476.4396
Epoch [173/200], Loss: 476.2837
Epoch [174/200], Loss: 476.3421
Epoch [175/200], Loss: 476.3682
Epoch [176/200], Loss: 476.3225
Epoch [177/200], Loss: 476.3406
Epoch [178/200], Loss: 476.3830
Epoch [179/200], Loss: 476.3720
Epoch [180/200], Loss: 476.2376
Epoch [181/200], Loss: 476.2954
Epoch [182/200], Loss: 476.3222
Epoch [183/200], Loss: 476.3124
Epoch [184/200], Loss: 476.2557
Epoch [185/200], Loss: 476.2826
Epoch [186/200], Loss: 476.3063
Epoch [187/200], Loss: 476.2557
Epoch [188/200], Loss: 476.3076
Epoch [189/200], Loss: 476.2361
Epoch [190/200], Loss: 476.2280
Epoch [191/200], Loss: 476.2316
Epoch [192/200], Loss: 476.2377
Epoch [193/200], Loss: 476.2457
Epoch [194/200], Loss: 476.1917
Epoch [195/200], Loss: 476.2527
Epoch [196/200], Loss: 476.2363
Epoch [197/200], Loss: 476.1386
Epoch [198/200], Loss: 476.1677
Epoch [199/200], Loss: 476.0876
Epoch [200/200], Loss: 476.1694