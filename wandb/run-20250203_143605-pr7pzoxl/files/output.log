/home/yec23006/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/yec23006/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/yec23006/.conda/envs/torch121/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
Epoch [1/200], Loss: 1456.7278
Epoch [2/200], Loss: 479.4815
Epoch [3/200], Loss: 479.0770
Epoch [4/200], Loss: 478.6597
Epoch [5/200], Loss: 478.6861
Epoch [6/200], Loss: 478.5669
Epoch [7/200], Loss: 478.5300
Epoch [8/200], Loss: 478.4584
Epoch [9/200], Loss: 478.3906
Epoch [10/200], Loss: 478.3174
Epoch [11/200], Loss: 478.2672
Epoch [12/200], Loss: 478.2404
Epoch [13/200], Loss: 478.1924
Epoch [14/200], Loss: 478.2863
Epoch [15/200], Loss: 478.1348
Epoch [16/200], Loss: 478.1157
Epoch [17/200], Loss: 478.0880
Epoch [18/200], Loss: 478.0372
Epoch [19/200], Loss: 477.9920
Epoch [20/200], Loss: 477.9559
Epoch [21/200], Loss: 477.9481
Epoch [22/200], Loss: 477.9645
Epoch [23/200], Loss: 477.8997
Epoch [24/200], Loss: 477.8935
Epoch [25/200], Loss: 477.8959
Epoch [26/200], Loss: 477.8237
Epoch [27/200], Loss: 477.8893
Epoch [28/200], Loss: 477.7916
Epoch [29/200], Loss: 477.7999
Epoch [30/200], Loss: 477.7488
Epoch [31/200], Loss: 477.7821
Epoch [32/200], Loss: 477.7495
Epoch [33/200], Loss: 477.7172
Epoch [34/200], Loss: 477.7550
Epoch [35/200], Loss: 477.6912
Epoch [36/200], Loss: 477.7148
Epoch [37/200], Loss: 477.6843
Epoch [38/200], Loss: 477.6422
Epoch [39/200], Loss: 477.6656
Epoch [40/200], Loss: 477.6859
Epoch [41/200], Loss: 477.6475
Epoch [42/200], Loss: 477.6359
Epoch [43/200], Loss: 477.6375
Epoch [44/200], Loss: 477.5709
Epoch [45/200], Loss: 477.6263
Epoch [46/200], Loss: 477.5897
Epoch [47/200], Loss: 477.5869
Epoch [48/200], Loss: 477.5679
Epoch [49/200], Loss: 477.4829
Epoch [50/200], Loss: 477.5556
Epoch [51/200], Loss: 477.4729
Epoch [52/200], Loss: 477.3763
Epoch [53/200], Loss: 477.3011
Epoch [54/200], Loss: 477.2780
Epoch [55/200], Loss: 477.2008
Epoch [56/200], Loss: 477.1899
Epoch [57/200], Loss: 477.1315
Epoch [58/200], Loss: 477.1535
Epoch [59/200], Loss: 477.0552
Epoch [60/200], Loss: 477.1168
Epoch [61/200], Loss: 477.0304
Epoch [62/200], Loss: 477.0112
Epoch [63/200], Loss: 476.9907
Epoch [64/200], Loss: 476.9301
Epoch [65/200], Loss: 477.0012
Epoch [66/200], Loss: 476.9012
Epoch [67/200], Loss: 476.9158
Epoch [68/200], Loss: 476.8875
Epoch [69/200], Loss: 476.8121
Epoch [70/200], Loss: 476.7822
Epoch [71/200], Loss: 476.7761
Epoch [72/200], Loss: 476.7906
Epoch [73/200], Loss: 476.7147
Epoch [74/200], Loss: 476.7139
Epoch [75/200], Loss: 476.6765
Epoch [76/200], Loss: 476.6151
Epoch [77/200], Loss: 476.6237
Epoch [78/200], Loss: 476.5847
Epoch [79/200], Loss: 476.5750
Epoch [80/200], Loss: 476.6634
Epoch [81/200], Loss: 476.5676
Epoch [82/200], Loss: 476.5538
Epoch [83/200], Loss: 476.5631
Epoch [84/200], Loss: 476.4991
Epoch [85/200], Loss: 476.5352
Epoch [86/200], Loss: 476.4306
Epoch [87/200], Loss: 476.4379
Epoch [88/200], Loss: 476.4439
Epoch [89/200], Loss: 476.4778
Epoch [90/200], Loss: 476.4323
Epoch [91/200], Loss: 476.4337
Epoch [92/200], Loss: 476.3698
Epoch [93/200], Loss: 476.3192
Epoch [94/200], Loss: 476.3239
Epoch [95/200], Loss: 476.4378
Epoch [96/200], Loss: 476.3019
Epoch [97/200], Loss: 476.2292
Epoch [98/200], Loss: 476.2352
Epoch [99/200], Loss: 476.2640
Epoch [100/200], Loss: 476.2834
Epoch [101/200], Loss: 476.2153
Epoch [102/200], Loss: 476.1890
Epoch [103/200], Loss: 476.2023
Epoch [104/200], Loss: 476.1551
Epoch [105/200], Loss: 476.1649
Epoch [106/200], Loss: 476.1106
Epoch [107/200], Loss: 476.1063
Epoch [108/200], Loss: 476.1714
Epoch [109/200], Loss: 476.0969
Epoch [110/200], Loss: 476.1392
Epoch [111/200], Loss: 476.1376
Epoch [112/200], Loss: 476.1606
Epoch [113/200], Loss: 476.0473
Epoch [114/200], Loss: 475.9631
Epoch [115/200], Loss: 476.0749
Epoch [116/200], Loss: 475.9812
Epoch [117/200], Loss: 476.0087
Epoch [118/200], Loss: 475.9497
Epoch [119/200], Loss: 476.0308
Epoch [120/200], Loss: 475.9354
Epoch [121/200], Loss: 475.8807
Epoch [122/200], Loss: 475.9610
Epoch [123/200], Loss: 475.8632
Epoch [124/200], Loss: 475.8221
Epoch [125/200], Loss: 475.9271
Epoch [126/200], Loss: 475.8308
Epoch [127/200], Loss: 475.8186
Epoch [128/200], Loss: 475.7716
Epoch [129/200], Loss: 475.7632
Epoch [130/200], Loss: 475.9728
Epoch [131/200], Loss: 475.8996
Epoch [132/200], Loss: 475.7131
Epoch [133/200], Loss: 475.7163
Epoch [134/200], Loss: 475.7690
Epoch [135/200], Loss: 475.7605
Epoch [136/200], Loss: 475.7026
Epoch [137/200], Loss: 475.7017
Epoch [138/200], Loss: 475.7209
Epoch [139/200], Loss: 475.6031
Epoch [140/200], Loss: 475.6812
Epoch [141/200], Loss: 475.5914
Epoch [142/200], Loss: 475.6210
Epoch [143/200], Loss: 475.6742
Epoch [144/200], Loss: 475.5199
Epoch [145/200], Loss: 475.5755
Epoch [146/200], Loss: 475.5977
Epoch [147/200], Loss: 475.5895
Epoch [148/200], Loss: 475.5577
Epoch [149/200], Loss: 475.5735
Epoch [150/200], Loss: 475.5253
Epoch [151/200], Loss: 475.5502
Epoch [152/200], Loss: 475.4989
Epoch [153/200], Loss: 475.4654
Epoch [154/200], Loss: 475.5565
Epoch [155/200], Loss: 475.4439
Epoch [156/200], Loss: 475.4675
Epoch [157/200], Loss: 475.4408
Epoch [158/200], Loss: 475.3546
Epoch [159/200], Loss: 475.3453
Epoch [160/200], Loss: 475.3992
Epoch [161/200], Loss: 475.3097
Epoch [162/200], Loss: 475.4085
Epoch [163/200], Loss: 475.3143
Epoch [164/200], Loss: 475.2909
Epoch [165/200], Loss: 475.3780
Epoch [166/200], Loss: 475.3262
Epoch [167/200], Loss: 475.3553
Epoch [168/200], Loss: 475.3625
Epoch [169/200], Loss: 475.2994
Epoch [170/200], Loss: 475.2656
Epoch [171/200], Loss: 475.2441
Epoch [172/200], Loss: 475.2337
Epoch [173/200], Loss: 475.2986
Epoch [174/200], Loss: 475.1921
Epoch [175/200], Loss: 475.3253
Epoch [176/200], Loss: 475.2989
Epoch [177/200], Loss: 475.1575
Epoch [178/200], Loss: 475.1158
Epoch [179/200], Loss: 475.1594
Epoch [180/200], Loss: 475.2573
Epoch [181/200], Loss: 475.1768
Epoch [182/200], Loss: 475.2171
Epoch [183/200], Loss: 475.2151
Epoch [184/200], Loss: 475.0563
Epoch [185/200], Loss: 475.2546
Epoch [186/200], Loss: 475.1423
Epoch [187/200], Loss: 475.2550
Epoch [188/200], Loss: 475.1606
Epoch [189/200], Loss: 475.1344
Epoch [190/200], Loss: 475.1180
Epoch [191/200], Loss: 475.0860
Epoch [192/200], Loss: 475.1389
Epoch [193/200], Loss: 475.2390
Epoch [194/200], Loss: 475.0970
Epoch [195/200], Loss: 475.1468
Epoch [196/200], Loss: 474.9892
Epoch [197/200], Loss: 475.0304
Epoch [198/200], Loss: 475.0866
Epoch [199/200], Loss: 475.1776
Epoch [200/200], Loss: 474.9690