/home/yec23006/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/yec23006/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/yec23006/.conda/envs/torch121/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
Epoch [1/200], Loss: 1500.8147
Epoch [2/200], Loss: 479.3978
Epoch [3/200], Loss: 479.1413
Epoch [4/200], Loss: 478.9422
Epoch [5/200], Loss: 478.7023
Epoch [6/200], Loss: 478.4849
Epoch [7/200], Loss: 478.5399
Epoch [8/200], Loss: 478.4049
Epoch [9/200], Loss: 478.3290
Epoch [10/200], Loss: 478.3552
Epoch [11/200], Loss: 478.2594
Epoch [12/200], Loss: 478.2665
Epoch [13/200], Loss: 478.1859
Epoch [14/200], Loss: 478.1196
Epoch [15/200], Loss: 478.1596
Epoch [16/200], Loss: 478.0862
Epoch [17/200], Loss: 478.0374
Epoch [18/200], Loss: 478.0052
Epoch [19/200], Loss: 478.0007
Epoch [20/200], Loss: 477.9801
Epoch [21/200], Loss: 477.9310
Epoch [22/200], Loss: 477.8992
Epoch [23/200], Loss: 477.8754
Epoch [24/200], Loss: 477.8734
Epoch [25/200], Loss: 477.8572
Epoch [26/200], Loss: 477.8080
Epoch [27/200], Loss: 477.7969
Epoch [28/200], Loss: 477.7588
Epoch [29/200], Loss: 477.7700
Epoch [30/200], Loss: 477.7222
Epoch [31/200], Loss: 477.7380
Epoch [32/200], Loss: 477.7138
Epoch [33/200], Loss: 477.6693
Epoch [34/200], Loss: 477.6945
Epoch [35/200], Loss: 477.6754
Epoch [36/200], Loss: 477.6517
Epoch [37/200], Loss: 477.6754
Epoch [38/200], Loss: 477.6552
Epoch [39/200], Loss: 477.6336
Epoch [40/200], Loss: 477.5857
Epoch [41/200], Loss: 477.5529
Epoch [42/200], Loss: 477.5950
Epoch [43/200], Loss: 477.5801
Epoch [44/200], Loss: 477.5891
Epoch [45/200], Loss: 477.5383
Epoch [46/200], Loss: 477.5676
Epoch [47/200], Loss: 477.5521
Epoch [48/200], Loss: 477.5489
Epoch [49/200], Loss: 477.5407
Epoch [50/200], Loss: 477.5262
Epoch [51/200], Loss: 477.5042
Epoch [52/200], Loss: 477.4487
Epoch [53/200], Loss: 477.4796
Epoch [54/200], Loss: 477.5119
Epoch [55/200], Loss: 477.4480
Epoch [56/200], Loss: 477.4293
Epoch [57/200], Loss: 477.4507
Epoch [58/200], Loss: 477.4480
Epoch [59/200], Loss: 477.4296
Epoch [60/200], Loss: 477.4336
Epoch [61/200], Loss: 477.4710
Epoch [62/200], Loss: 477.4614
Epoch [63/200], Loss: 477.3646
Epoch [64/200], Loss: 477.3786
Epoch [65/200], Loss: 477.3400
Epoch [66/200], Loss: 477.3610
Epoch [67/200], Loss: 477.3723
Epoch [68/200], Loss: 477.3885
Epoch [69/200], Loss: 477.3614
Epoch [70/200], Loss: 477.3742
Epoch [71/200], Loss: 477.3105
Epoch [72/200], Loss: 477.3406
Epoch [73/200], Loss: 477.3219
Epoch [74/200], Loss: 477.2897
Epoch [75/200], Loss: 477.3055
Epoch [76/200], Loss: 477.2814
Epoch [77/200], Loss: 477.2504
Epoch [78/200], Loss: 477.2848
Epoch [79/200], Loss: 477.2345
Epoch [80/200], Loss: 477.2172
Epoch [81/200], Loss: 477.2616
Epoch [82/200], Loss: 477.2251
Epoch [83/200], Loss: 477.2292
Epoch [84/200], Loss: 477.2031
Epoch [85/200], Loss: 477.2162
Epoch [86/200], Loss: 477.1859
Epoch [87/200], Loss: 477.1505
Epoch [88/200], Loss: 477.1604
Epoch [89/200], Loss: 477.1258
Epoch [90/200], Loss: 477.1204
Epoch [91/200], Loss: 477.0902
Epoch [92/200], Loss: 477.0137
Epoch [93/200], Loss: 477.0739
Epoch [94/200], Loss: 477.1036
Epoch [95/200], Loss: 477.0010
Epoch [96/200], Loss: 477.0299
Epoch [97/200], Loss: 476.9582
Epoch [98/200], Loss: 477.0366
Epoch [99/200], Loss: 476.9774
Epoch [100/200], Loss: 476.9814
Epoch [101/200], Loss: 476.9851
Epoch [102/200], Loss: 477.0161
Epoch [103/200], Loss: 476.9474
Epoch [104/200], Loss: 476.9346
Epoch [105/200], Loss: 476.9096
Epoch [106/200], Loss: 476.9271
Epoch [107/200], Loss: 476.9063
Epoch [108/200], Loss: 476.8692
Epoch [109/200], Loss: 476.8527
Epoch [110/200], Loss: 476.8667
Epoch [111/200], Loss: 476.8464
Epoch [112/200], Loss: 476.8361
Epoch [113/200], Loss: 476.8027
Epoch [114/200], Loss: 476.8147
Epoch [115/200], Loss: 476.7963
Epoch [116/200], Loss: 476.7958
Epoch [117/200], Loss: 476.7928
Epoch [118/200], Loss: 476.7393
Epoch [119/200], Loss: 476.7979
Epoch [120/200], Loss: 476.7585
Epoch [121/200], Loss: 476.7847
Epoch [122/200], Loss: 476.7864
Epoch [123/200], Loss: 476.7394
Epoch [124/200], Loss: 476.7312
Epoch [125/200], Loss: 476.7571
Epoch [126/200], Loss: 476.7335
Epoch [127/200], Loss: 476.6521
Epoch [128/200], Loss: 476.6771
Epoch [129/200], Loss: 476.7111
Epoch [130/200], Loss: 476.6868
Epoch [131/200], Loss: 476.6376
Epoch [132/200], Loss: 476.7229
Epoch [133/200], Loss: 476.6602
Epoch [134/200], Loss: 476.5971
Epoch [135/200], Loss: 476.6660
Epoch [136/200], Loss: 476.6253
Epoch [137/200], Loss: 476.5531
Epoch [138/200], Loss: 476.6662
Epoch [139/200], Loss: 476.5571
Epoch [140/200], Loss: 476.5519
Epoch [141/200], Loss: 476.5518
Epoch [142/200], Loss: 476.5484
Epoch [143/200], Loss: 476.5347
Epoch [144/200], Loss: 476.4738
Epoch [145/200], Loss: 476.5392
Epoch [146/200], Loss: 476.5961
Epoch [147/200], Loss: 476.5717
Epoch [148/200], Loss: 476.5048
Epoch [149/200], Loss: 476.5346
Epoch [150/200], Loss: 476.5320
Epoch [151/200], Loss: 476.4565
Epoch [152/200], Loss: 476.4887
Epoch [153/200], Loss: 476.4970
Epoch [154/200], Loss: 476.4409
Epoch [155/200], Loss: 476.4354
Epoch [156/200], Loss: 476.4584
Epoch [157/200], Loss: 476.4266
Epoch [158/200], Loss: 476.4751
Epoch [159/200], Loss: 476.4433
Epoch [160/200], Loss: 476.3997
Epoch [161/200], Loss: 476.4589
Epoch [162/200], Loss: 476.3887
Epoch [163/200], Loss: 476.4233
Epoch [164/200], Loss: 476.3353
Epoch [165/200], Loss: 476.3962
Epoch [166/200], Loss: 476.3826
Epoch [167/200], Loss: 476.3822
Epoch [168/200], Loss: 476.3848
Epoch [169/200], Loss: 476.3722
Epoch [170/200], Loss: 476.3884
Epoch [171/200], Loss: 476.3232
Epoch [172/200], Loss: 476.3430
Epoch [173/200], Loss: 476.4272
Epoch [174/200], Loss: 476.3663
Epoch [175/200], Loss: 476.3410
Epoch [176/200], Loss: 476.3216
Epoch [177/200], Loss: 476.3129
Epoch [178/200], Loss: 476.3039
Epoch [179/200], Loss: 476.2727
Epoch [180/200], Loss: 476.3074
Epoch [181/200], Loss: 476.2999
Epoch [182/200], Loss: 476.2875
Epoch [183/200], Loss: 476.2496
Epoch [184/200], Loss: 476.3183
Epoch [185/200], Loss: 476.2719
Epoch [186/200], Loss: 476.2881
Epoch [187/200], Loss: 476.2277
Epoch [188/200], Loss: 476.2299
Epoch [189/200], Loss: 476.2747
Epoch [190/200], Loss: 476.2275
Epoch [191/200], Loss: 476.1946
Epoch [192/200], Loss: 476.2998
Epoch [193/200], Loss: 476.2665
Epoch [194/200], Loss: 476.2643
Epoch [195/200], Loss: 476.2341
Epoch [196/200], Loss: 476.2427
Epoch [197/200], Loss: 476.2275
Epoch [198/200], Loss: 476.1812
Epoch [199/200], Loss: 476.2792
Epoch [200/200], Loss: 476.2479