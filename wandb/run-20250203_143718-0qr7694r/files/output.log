/home/yec23006/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/yec23006/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/yec23006/.conda/envs/torch121/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
Epoch [1/200], Loss: 2633.9864
Epoch [2/200], Loss: 482.2255
Epoch [3/200], Loss: 479.9249
Epoch [4/200], Loss: 478.9773
Epoch [5/200], Loss: 478.8860
Epoch [6/200], Loss: 478.8722
Epoch [7/200], Loss: 478.6556
Epoch [8/200], Loss: 478.5519
Epoch [9/200], Loss: 478.6175
Epoch [10/200], Loss: 478.4894
Epoch [11/200], Loss: 478.4753
Epoch [12/200], Loss: 478.4403
Epoch [13/200], Loss: 478.4301
Epoch [14/200], Loss: 478.2927
Epoch [15/200], Loss: 478.2033
Epoch [16/200], Loss: 478.2439
Epoch [17/200], Loss: 478.1470
Epoch [18/200], Loss: 478.1502
Epoch [19/200], Loss: 478.1220
Epoch [20/200], Loss: 478.1446
Epoch [21/200], Loss: 478.0863
Epoch [22/200], Loss: 478.0501
Epoch [23/200], Loss: 477.9980
Epoch [24/200], Loss: 478.0011
Epoch [25/200], Loss: 477.9814
Epoch [26/200], Loss: 477.9816
Epoch [27/200], Loss: 477.9662
Epoch [28/200], Loss: 477.9319
Epoch [29/200], Loss: 477.8782
Epoch [30/200], Loss: 477.8936
Epoch [31/200], Loss: 477.8233
Epoch [32/200], Loss: 477.8034
Epoch [33/200], Loss: 477.7767
Epoch [34/200], Loss: 477.7725
Epoch [35/200], Loss: 477.7481
Epoch [36/200], Loss: 477.7659
Epoch [37/200], Loss: 477.7514
Epoch [38/200], Loss: 477.7340
Epoch [39/200], Loss: 477.7446
Epoch [40/200], Loss: 477.7062
Epoch [41/200], Loss: 477.7380
Epoch [42/200], Loss: 477.6742
Epoch [43/200], Loss: 477.7219
Epoch [44/200], Loss: 477.6356
Epoch [45/200], Loss: 477.6285
Epoch [46/200], Loss: 477.6318
Epoch [47/200], Loss: 477.6195
Epoch [48/200], Loss: 477.6176
Epoch [49/200], Loss: 477.6259
Epoch [50/200], Loss: 477.5734
Epoch [51/200], Loss: 477.6121
Epoch [52/200], Loss: 477.6133
Epoch [53/200], Loss: 477.5751
Epoch [54/200], Loss: 477.5983
Epoch [55/200], Loss: 477.5648
Epoch [56/200], Loss: 477.5876
Epoch [57/200], Loss: 477.5915
Epoch [58/200], Loss: 477.5478
Epoch [59/200], Loss: 477.5219
Epoch [60/200], Loss: 477.5151
Epoch [61/200], Loss: 477.5556
Epoch [62/200], Loss: 477.5234
Epoch [63/200], Loss: 477.4810
Epoch [64/200], Loss: 477.4848
Epoch [65/200], Loss: 477.4850
Epoch [66/200], Loss: 477.4658
Epoch [67/200], Loss: 477.4958
Epoch [68/200], Loss: 477.4575
Epoch [69/200], Loss: 477.3999
Epoch [70/200], Loss: 477.4513
Epoch [71/200], Loss: 477.4425
Epoch [72/200], Loss: 477.3942
Epoch [73/200], Loss: 477.4285
Epoch [74/200], Loss: 477.4321
Epoch [75/200], Loss: 477.4130
Epoch [76/200], Loss: 477.4009
Epoch [77/200], Loss: 477.4002
Epoch [78/200], Loss: 477.3559
Epoch [79/200], Loss: 477.3885
Epoch [80/200], Loss: 477.3313
Epoch [81/200], Loss: 477.3624
Epoch [82/200], Loss: 477.3699
Epoch [83/200], Loss: 477.3115
Epoch [84/200], Loss: 477.3588
Epoch [85/200], Loss: 477.3265
Epoch [86/200], Loss: 477.3069
Epoch [87/200], Loss: 477.3081
Epoch [88/200], Loss: 477.3365
Epoch [89/200], Loss: 477.2974
Epoch [90/200], Loss: 477.2603
Epoch [91/200], Loss: 477.3249
Epoch [92/200], Loss: 477.2878
Epoch [93/200], Loss: 477.2389
Epoch [94/200], Loss: 477.2501
Epoch [95/200], Loss: 477.2616
Epoch [96/200], Loss: 477.2444
Epoch [97/200], Loss: 477.2483
Epoch [98/200], Loss: 477.2259
Epoch [99/200], Loss: 477.2391
Epoch [100/200], Loss: 477.2405
Epoch [101/200], Loss: 477.2283
Epoch [102/200], Loss: 477.1972
Epoch [103/200], Loss: 477.1779
Epoch [104/200], Loss: 477.2550
Epoch [105/200], Loss: 477.1897
Epoch [106/200], Loss: 477.2408
Epoch [107/200], Loss: 477.2321
Epoch [108/200], Loss: 477.1582
Epoch [109/200], Loss: 477.1619
Epoch [110/200], Loss: 477.1723
Epoch [111/200], Loss: 477.1677
Epoch [112/200], Loss: 477.1655
Epoch [113/200], Loss: 477.1300
Epoch [114/200], Loss: 477.1160
Epoch [115/200], Loss: 477.1751
Epoch [116/200], Loss: 477.1646
Epoch [117/200], Loss: 477.1540
Epoch [118/200], Loss: 477.1426
Epoch [119/200], Loss: 477.1098
Epoch [120/200], Loss: 477.1244
Epoch [121/200], Loss: 477.0660
Epoch [122/200], Loss: 477.1534
Epoch [123/200], Loss: 477.1145
Epoch [124/200], Loss: 477.0784
Epoch [125/200], Loss: 477.0606
Epoch [126/200], Loss: 477.0637
Epoch [127/200], Loss: 477.0525
Epoch [128/200], Loss: 477.0798
Epoch [129/200], Loss: 477.0120
Epoch [130/200], Loss: 477.0210
Epoch [131/200], Loss: 477.0064
Epoch [132/200], Loss: 477.0417
Epoch [133/200], Loss: 477.0185
Epoch [134/200], Loss: 477.0130
Epoch [135/200], Loss: 476.9597
Epoch [136/200], Loss: 477.0369
Epoch [137/200], Loss: 476.9568
Epoch [138/200], Loss: 476.9519
Epoch [139/200], Loss: 477.0156
Epoch [140/200], Loss: 477.0385
Epoch [141/200], Loss: 476.9840
Epoch [142/200], Loss: 476.9204
Epoch [143/200], Loss: 476.9077
Epoch [144/200], Loss: 476.9281
Epoch [145/200], Loss: 476.9058
Epoch [146/200], Loss: 476.9217
Epoch [147/200], Loss: 476.9499
Epoch [148/200], Loss: 476.9696
Epoch [149/200], Loss: 476.9239
Epoch [150/200], Loss: 476.8954
Epoch [151/200], Loss: 476.9334
Epoch [152/200], Loss: 476.9114
Epoch [153/200], Loss: 476.9190
Epoch [154/200], Loss: 476.9424
Epoch [155/200], Loss: 476.9569
Epoch [156/200], Loss: 476.8779
Epoch [157/200], Loss: 476.8806
Epoch [158/200], Loss: 476.8993
Epoch [159/200], Loss: 476.9000
Epoch [160/200], Loss: 476.7791
Epoch [161/200], Loss: 476.8239
Epoch [162/200], Loss: 476.7989
Epoch [163/200], Loss: 476.8109
Epoch [164/200], Loss: 476.7844
Epoch [165/200], Loss: 476.8290
Epoch [166/200], Loss: 476.7743
Epoch [167/200], Loss: 476.7935
Epoch [168/200], Loss: 476.8137
Epoch [169/200], Loss: 476.8555
Epoch [170/200], Loss: 476.7707
Epoch [171/200], Loss: 476.8487
Epoch [172/200], Loss: 476.8770
Epoch [173/200], Loss: 476.7841
Epoch [174/200], Loss: 476.7619
Epoch [175/200], Loss: 476.7758
Epoch [176/200], Loss: 476.7471
Epoch [177/200], Loss: 476.7416
Epoch [178/200], Loss: 476.7697
Epoch [179/200], Loss: 476.6980
Epoch [180/200], Loss: 476.6592
Epoch [181/200], Loss: 476.6366
Epoch [182/200], Loss: 476.6408
Epoch [183/200], Loss: 476.6578
Epoch [184/200], Loss: 476.5681
Epoch [185/200], Loss: 476.6413
Epoch [186/200], Loss: 476.6467
Epoch [187/200], Loss: 476.6175
Epoch [188/200], Loss: 476.6523
Epoch [189/200], Loss: 476.5923
Epoch [190/200], Loss: 476.6188
Epoch [191/200], Loss: 476.6147
Epoch [192/200], Loss: 476.6042
Epoch [193/200], Loss: 476.5593
Epoch [194/200], Loss: 476.5573
Epoch [195/200], Loss: 476.5646
Epoch [196/200], Loss: 476.5427
Epoch [197/200], Loss: 476.5535
Epoch [198/200], Loss: 476.5504
Epoch [199/200], Loss: 476.5396
Epoch [200/200], Loss: 476.4802