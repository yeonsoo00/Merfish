/home/yec23006/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.
  warnings.warn(
/home/yec23006/.local/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
/home/yec23006/.conda/envs/torch121/lib/python3.11/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).
  from pandas.core import (
Epoch [1/200], Loss: 2466.1455
Epoch [2/200], Loss: 482.3041
Epoch [3/200], Loss: 479.7914
Epoch [4/200], Loss: 479.7685
Epoch [5/200], Loss: 479.1930
Epoch [6/200], Loss: 479.0698
Epoch [7/200], Loss: 479.0780
Epoch [8/200], Loss: 478.7034
Epoch [9/200], Loss: 478.7902
Epoch [10/200], Loss: 478.6309
Epoch [11/200], Loss: 478.4459
Epoch [12/200], Loss: 478.4851
Epoch [13/200], Loss: 478.3168
Epoch [14/200], Loss: 478.3236
Epoch [15/200], Loss: 478.2440
Epoch [16/200], Loss: 478.2521
Epoch [17/200], Loss: 478.1289
Epoch [18/200], Loss: 478.1638
Epoch [19/200], Loss: 478.1260
Epoch [20/200], Loss: 478.1308
Epoch [21/200], Loss: 478.0430
Epoch [22/200], Loss: 477.9972
Epoch [23/200], Loss: 478.1128
Epoch [24/200], Loss: 477.9726
Epoch [25/200], Loss: 478.0043
Epoch [26/200], Loss: 477.8953
Epoch [27/200], Loss: 477.8881
Epoch [28/200], Loss: 477.9188
Epoch [29/200], Loss: 477.9137
Epoch [30/200], Loss: 477.8291
Epoch [31/200], Loss: 477.8314
Epoch [32/200], Loss: 477.8537
Epoch [33/200], Loss: 477.7445
Epoch [34/200], Loss: 477.7110
Epoch [35/200], Loss: 477.8358
Epoch [36/200], Loss: 477.6944
Epoch [37/200], Loss: 477.7004
Epoch [38/200], Loss: 477.7344
Epoch [39/200], Loss: 477.6748
Epoch [40/200], Loss: 477.7395
Epoch [41/200], Loss: 477.6155
Epoch [42/200], Loss: 477.6665
Epoch [43/200], Loss: 477.6012
Epoch [44/200], Loss: 477.6098
Epoch [45/200], Loss: 477.5800
Epoch [46/200], Loss: 477.6164
Epoch [47/200], Loss: 477.5812
Epoch [48/200], Loss: 477.6198
Epoch [49/200], Loss: 477.5487
Epoch [50/200], Loss: 477.5773
Epoch [51/200], Loss: 477.5436
Epoch [52/200], Loss: 477.5365
Epoch [53/200], Loss: 477.4883
Epoch [54/200], Loss: 477.4897
Epoch [55/200], Loss: 477.4896
Epoch [56/200], Loss: 477.4822
Epoch [57/200], Loss: 477.5187
Epoch [58/200], Loss: 477.4797
Epoch [59/200], Loss: 477.4431
Epoch [60/200], Loss: 477.4695
Epoch [61/200], Loss: 477.4043
Epoch [62/200], Loss: 477.4162
Epoch [63/200], Loss: 477.4413
Epoch [64/200], Loss: 477.4073
Epoch [65/200], Loss: 477.4188
Epoch [66/200], Loss: 477.4224
Epoch [67/200], Loss: 477.4170
Epoch [68/200], Loss: 477.3757
Epoch [69/200], Loss: 477.3708
Epoch [70/200], Loss: 477.3442
Epoch [71/200], Loss: 477.3217
Epoch [72/200], Loss: 477.3558
Epoch [73/200], Loss: 477.3606
Epoch [74/200], Loss: 477.3151
Epoch [75/200], Loss: 477.3458
Epoch [76/200], Loss: 477.3367
Epoch [77/200], Loss: 477.2736
Epoch [78/200], Loss: 477.2686
Epoch [79/200], Loss: 477.2809
Epoch [80/200], Loss: 477.2798
Epoch [81/200], Loss: 477.2760
Epoch [82/200], Loss: 477.2183
Epoch [83/200], Loss: 477.2240
Epoch [84/200], Loss: 477.2034
Epoch [85/200], Loss: 477.2591
Epoch [86/200], Loss: 477.2424
Epoch [87/200], Loss: 477.1904
Epoch [88/200], Loss: 477.2298
Epoch [89/200], Loss: 477.2483
Epoch [90/200], Loss: 477.2032
Epoch [91/200], Loss: 477.2129
Epoch [92/200], Loss: 477.2016
Epoch [93/200], Loss: 477.1560
Epoch [94/200], Loss: 477.2397
Epoch [95/200], Loss: 477.1660
Epoch [96/200], Loss: 477.1465
Epoch [97/200], Loss: 477.1724
Epoch [98/200], Loss: 477.1276
Epoch [99/200], Loss: 477.1633
Epoch [100/200], Loss: 477.1906
Epoch [101/200], Loss: 477.1323
Epoch [102/200], Loss: 477.1181
Epoch [103/200], Loss: 477.1248
Epoch [104/200], Loss: 477.1120
Epoch [105/200], Loss: 477.0749
Epoch [106/200], Loss: 477.0991
Epoch [107/200], Loss: 477.1213
Epoch [108/200], Loss: 477.1233
Epoch [109/200], Loss: 477.1121
Epoch [110/200], Loss: 477.0493
Epoch [111/200], Loss: 477.0402
Epoch [112/200], Loss: 477.0850
Epoch [113/200], Loss: 477.0534
Epoch [114/200], Loss: 477.0016
Epoch [115/200], Loss: 477.0205
Epoch [116/200], Loss: 477.0088
Epoch [117/200], Loss: 477.0363
Epoch [118/200], Loss: 476.9880
Epoch [119/200], Loss: 476.9627
Epoch [120/200], Loss: 476.9675
Epoch [121/200], Loss: 476.9640
Epoch [122/200], Loss: 476.9933
Epoch [123/200], Loss: 477.0102
Epoch [124/200], Loss: 476.9588
Epoch [125/200], Loss: 476.9139
Epoch [126/200], Loss: 476.9482
Epoch [127/200], Loss: 476.9477
Epoch [128/200], Loss: 476.9479
Epoch [129/200], Loss: 476.8518
Epoch [130/200], Loss: 476.8991
Epoch [131/200], Loss: 476.8874
Epoch [132/200], Loss: 476.8729
Epoch [133/200], Loss: 476.8452
Epoch [134/200], Loss: 476.8636
Epoch [135/200], Loss: 476.8878
Epoch [136/200], Loss: 476.8895
Epoch [137/200], Loss: 476.8159
Epoch [138/200], Loss: 476.8655
Epoch [139/200], Loss: 476.7953
Epoch [140/200], Loss: 476.8596
Epoch [141/200], Loss: 476.7773
Epoch [142/200], Loss: 476.8222
Epoch [143/200], Loss: 476.7947
Epoch [144/200], Loss: 476.7593
Epoch [145/200], Loss: 476.7254
Epoch [146/200], Loss: 476.8298
Epoch [147/200], Loss: 476.7998
Epoch [148/200], Loss: 476.7421
Epoch [149/200], Loss: 476.7170
Epoch [150/200], Loss: 476.7196
Epoch [151/200], Loss: 476.7535
Epoch [152/200], Loss: 476.7552
Epoch [153/200], Loss: 476.6895
Epoch [154/200], Loss: 476.7098
Epoch [155/200], Loss: 476.7217
Epoch [156/200], Loss: 476.7617
Epoch [157/200], Loss: 476.7143
Epoch [158/200], Loss: 476.6482
Epoch [159/200], Loss: 476.6692
Epoch [160/200], Loss: 476.6629
Epoch [161/200], Loss: 476.6362
Epoch [162/200], Loss: 476.6398
Epoch [163/200], Loss: 476.6435
Epoch [164/200], Loss: 476.6327
Epoch [165/200], Loss: 476.6044
Epoch [166/200], Loss: 476.6474
Epoch [167/200], Loss: 476.5679
Epoch [168/200], Loss: 476.5617
Epoch [169/200], Loss: 476.6002
Epoch [170/200], Loss: 476.5230
Epoch [171/200], Loss: 476.5656
Epoch [172/200], Loss: 476.5940
Epoch [173/200], Loss: 476.6202
Epoch [174/200], Loss: 476.5429
Epoch [175/200], Loss: 476.5426
Epoch [176/200], Loss: 476.5419
Epoch [177/200], Loss: 476.5225
Epoch [178/200], Loss: 476.4712
Epoch [179/200], Loss: 476.5168
Epoch [180/200], Loss: 476.6119
Epoch [181/200], Loss: 476.4645
Epoch [182/200], Loss: 476.4199
Epoch [183/200], Loss: 476.4798
Epoch [184/200], Loss: 476.4826
Epoch [185/200], Loss: 476.5040
Epoch [186/200], Loss: 476.4616
Epoch [187/200], Loss: 476.4701
Epoch [188/200], Loss: 476.4505
Epoch [189/200], Loss: 476.4772
Epoch [190/200], Loss: 476.4166
Epoch [191/200], Loss: 476.4446
Epoch [192/200], Loss: 476.4661
Epoch [193/200], Loss: 476.3786
Epoch [194/200], Loss: 476.4807
Epoch [195/200], Loss: 476.4026
Epoch [196/200], Loss: 476.4081
Epoch [197/200], Loss: 476.4059
Epoch [198/200], Loss: 476.4099
Epoch [199/200], Loss: 476.3752
Epoch [200/200], Loss: 476.4053